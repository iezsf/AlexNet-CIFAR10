{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "cifar10_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9txw4Cfj-e2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "0xhONd4zj-e6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJAoah8wj-e9",
        "colab_type": "text"
      },
      "source": [
        "The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "We transform them to Tensors of normalized range [-1, 1].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeA_Qsh3j-e-",
        "colab_type": "code",
        "outputId": "d28701dc-0c1a-40aa-b916-11ec831fa817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:02, 71866968.90it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vWyeOmSj-fB",
        "colab_type": "text"
      },
      "source": [
        "Let us show some of the training images, for fun.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQqKJOz7j-fC",
        "colab_type": "code",
        "outputId": "48329604-667e-4c07-e122-fff0ccf7f51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxzaBf6F7eHh4bBP4F7qHh4fHNsH/D0+BSKg0LsXhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " ship  bird plane   car\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQla8LEbj-fE",
        "colab_type": "text"
      },
      "source": [
        "2. Define a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNvqqVdDj-fF",
        "colab_type": "code",
        "outputId": "a28cf221-936c-405b-db8b-1c6f84ee9536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2zTqvEJj-fI",
        "colab_type": "code",
        "outputId": "ea784595-5fe9-444f-bbe0-b085f3fc79ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "NUM_CLASSES = 10\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 2 * 2, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 2 * 2)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = AlexNet()\n",
        "net.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5)\n",
              "    (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace)\n",
              "    (3): Dropout(p=0.5)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WtaPz9fj-fK",
        "colab_type": "text"
      },
      "source": [
        "3. Define a Loss function and optimizer\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0xOdFx9j-fL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhUBinfvj-fO",
        "colab_type": "text"
      },
      "source": [
        "4. Train the network\n",
        "^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "This is when things start to get interesting.\n",
        "We simply have to loop over our data iterator, and feed the inputs to the\n",
        "network and optimize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwbzt5xGj-fP",
        "colab_type": "code",
        "outputId": "15e75c51-b5fc-45db-bb1d-42c0092c9a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        # inputs, labels = data\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.303\n",
            "[1,  4000] loss: 2.298\n",
            "[1,  6000] loss: 2.103\n",
            "[1,  8000] loss: 1.909\n",
            "[1, 10000] loss: 1.789\n",
            "[1, 12000] loss: 1.699\n",
            "[2,  2000] loss: 1.580\n",
            "[2,  4000] loss: 1.518\n",
            "[2,  6000] loss: 1.459\n",
            "[2,  8000] loss: 1.392\n",
            "[2, 10000] loss: 1.349\n",
            "[2, 12000] loss: 1.279\n",
            "[3,  2000] loss: 1.204\n",
            "[3,  4000] loss: 1.177\n",
            "[3,  6000] loss: 1.141\n",
            "[3,  8000] loss: 1.105\n",
            "[3, 10000] loss: 1.081\n",
            "[3, 12000] loss: 1.066\n",
            "[4,  2000] loss: 0.959\n",
            "[4,  4000] loss: 0.963\n",
            "[4,  6000] loss: 0.923\n",
            "[4,  8000] loss: 0.941\n",
            "[4, 10000] loss: 0.912\n",
            "[4, 12000] loss: 0.896\n",
            "[5,  2000] loss: 0.794\n",
            "[5,  4000] loss: 0.797\n",
            "[5,  6000] loss: 0.791\n",
            "[5,  8000] loss: 0.771\n",
            "[5, 10000] loss: 0.777\n",
            "[5, 12000] loss: 0.759\n",
            "[6,  2000] loss: 0.646\n",
            "[6,  4000] loss: 0.664\n",
            "[6,  6000] loss: 0.649\n",
            "[6,  8000] loss: 0.677\n",
            "[6, 10000] loss: 0.666\n",
            "[6, 12000] loss: 0.665\n",
            "[7,  2000] loss: 0.525\n",
            "[7,  4000] loss: 0.552\n",
            "[7,  6000] loss: 0.560\n",
            "[7,  8000] loss: 0.575\n",
            "[7, 10000] loss: 0.564\n",
            "[7, 12000] loss: 0.575\n",
            "[8,  2000] loss: 0.406\n",
            "[8,  4000] loss: 0.444\n",
            "[8,  6000] loss: 0.475\n",
            "[8,  8000] loss: 0.480\n",
            "[8, 10000] loss: 0.483\n",
            "[8, 12000] loss: 0.488\n",
            "[9,  2000] loss: 0.362\n",
            "[9,  4000] loss: 0.368\n",
            "[9,  6000] loss: 0.378\n",
            "[9,  8000] loss: 0.380\n",
            "[9, 10000] loss: 0.397\n",
            "[9, 12000] loss: 0.403\n",
            "[10,  2000] loss: 0.274\n",
            "[10,  4000] loss: 0.302\n",
            "[10,  6000] loss: 0.306\n",
            "[10,  8000] loss: 0.328\n",
            "[10, 10000] loss: 0.333\n",
            "[10, 12000] loss: 0.337\n",
            "[11,  2000] loss: 0.227\n",
            "[11,  4000] loss: 0.251\n",
            "[11,  6000] loss: 0.236\n",
            "[11,  8000] loss: 0.280\n",
            "[11, 10000] loss: 0.284\n",
            "[11, 12000] loss: 0.279\n",
            "[12,  2000] loss: 0.176\n",
            "[12,  4000] loss: 0.195\n",
            "[12,  6000] loss: 0.207\n",
            "[12,  8000] loss: 0.212\n",
            "[12, 10000] loss: 0.233\n",
            "[12, 12000] loss: 0.231\n",
            "[13,  2000] loss: 0.142\n",
            "[13,  4000] loss: 0.155\n",
            "[13,  6000] loss: 0.180\n",
            "[13,  8000] loss: 0.201\n",
            "[13, 10000] loss: 0.191\n",
            "[13, 12000] loss: 0.182\n",
            "[14,  2000] loss: 0.122\n",
            "[14,  4000] loss: 0.146\n",
            "[14,  6000] loss: 0.150\n",
            "[14,  8000] loss: 0.141\n",
            "[14, 10000] loss: 0.168\n",
            "[14, 12000] loss: 0.160\n",
            "[15,  2000] loss: 0.111\n",
            "[15,  4000] loss: 0.115\n",
            "[15,  6000] loss: 0.125\n",
            "[15,  8000] loss: 0.130\n",
            "[15, 10000] loss: 0.148\n",
            "[15, 12000] loss: 0.146\n",
            "[16,  2000] loss: 0.078\n",
            "[16,  4000] loss: 0.095\n",
            "[16,  6000] loss: 0.099\n",
            "[16,  8000] loss: 0.131\n",
            "[16, 10000] loss: 0.110\n",
            "[16, 12000] loss: 0.111\n",
            "[17,  2000] loss: 0.072\n",
            "[17,  4000] loss: 0.096\n",
            "[17,  6000] loss: 0.078\n",
            "[17,  8000] loss: 0.092\n",
            "[17, 10000] loss: 0.090\n",
            "[17, 12000] loss: 0.095\n",
            "[18,  2000] loss: 0.055\n",
            "[18,  4000] loss: 0.072\n",
            "[18,  6000] loss: 0.082\n",
            "[18,  8000] loss: 0.098\n",
            "[18, 10000] loss: 0.105\n",
            "[18, 12000] loss: 0.085\n",
            "[19,  2000] loss: 0.060\n",
            "[19,  4000] loss: 0.069\n",
            "[19,  6000] loss: 0.076\n",
            "[19,  8000] loss: 0.071\n",
            "[19, 10000] loss: 0.080\n",
            "[19, 12000] loss: 0.082\n",
            "[20,  2000] loss: 0.042\n",
            "[20,  4000] loss: 0.056\n",
            "[20,  6000] loss: 0.073\n",
            "[20,  8000] loss: 0.056\n",
            "[20, 10000] loss: 0.064\n",
            "[20, 12000] loss: 0.079\n",
            "[21,  2000] loss: 0.044\n",
            "[21,  4000] loss: 0.048\n",
            "[21,  6000] loss: 0.056\n",
            "[21,  8000] loss: 0.053\n",
            "[21, 10000] loss: 0.077\n",
            "[21, 12000] loss: 0.067\n",
            "[22,  2000] loss: 0.036\n",
            "[22,  4000] loss: 0.038\n",
            "[22,  6000] loss: 0.061\n",
            "[22,  8000] loss: 0.048\n",
            "[22, 10000] loss: 0.059\n",
            "[22, 12000] loss: 0.051\n",
            "[23,  2000] loss: 0.027\n",
            "[23,  4000] loss: 0.035\n",
            "[23,  6000] loss: 0.043\n",
            "[23,  8000] loss: 0.049\n",
            "[23, 10000] loss: 0.051\n",
            "[23, 12000] loss: 0.062\n",
            "[24,  2000] loss: 0.033\n",
            "[24,  4000] loss: 0.038\n",
            "[24,  6000] loss: 0.028\n",
            "[24,  8000] loss: 0.040\n",
            "[24, 10000] loss: 0.046\n",
            "[24, 12000] loss: 0.051\n",
            "[25,  2000] loss: 0.036\n",
            "[25,  4000] loss: 0.033\n",
            "[25,  6000] loss: 0.048\n",
            "[25,  8000] loss: 0.042\n",
            "[25, 10000] loss: 0.042\n",
            "[25, 12000] loss: 0.060\n",
            "[26,  2000] loss: 0.035\n",
            "[26,  4000] loss: 0.017\n",
            "[26,  6000] loss: 0.036\n",
            "[26,  8000] loss: 0.031\n",
            "[26, 10000] loss: 0.047\n",
            "[26, 12000] loss: 0.048\n",
            "[27,  2000] loss: 0.027\n",
            "[27,  4000] loss: 0.023\n",
            "[27,  6000] loss: 0.029\n",
            "[27,  8000] loss: 0.028\n",
            "[27, 10000] loss: 0.037\n",
            "[27, 12000] loss: 0.036\n",
            "[28,  2000] loss: 0.032\n",
            "[28,  4000] loss: 0.035\n",
            "[28,  6000] loss: 0.027\n",
            "[28,  8000] loss: 0.018\n",
            "[28, 10000] loss: 0.041\n",
            "[28, 12000] loss: 0.036\n",
            "[29,  2000] loss: 0.030\n",
            "[29,  4000] loss: 0.028\n",
            "[29,  6000] loss: 0.024\n",
            "[29,  8000] loss: 0.027\n",
            "[29, 10000] loss: 0.040\n",
            "[29, 12000] loss: 0.026\n",
            "[30,  2000] loss: 0.021\n",
            "[30,  4000] loss: 0.018\n",
            "[30,  6000] loss: 0.029\n",
            "[30,  8000] loss: 0.028\n",
            "[30, 10000] loss: 0.040\n",
            "[30, 12000] loss: 0.031\n",
            "[31,  2000] loss: 0.012\n",
            "[31,  4000] loss: 0.017\n",
            "[31,  6000] loss: 0.019\n",
            "[31,  8000] loss: 0.032\n",
            "[31, 10000] loss: 0.033\n",
            "[31, 12000] loss: 0.032\n",
            "[32,  2000] loss: 0.021\n",
            "[32,  4000] loss: 0.015\n",
            "[32,  6000] loss: 0.029\n",
            "[32,  8000] loss: 0.027\n",
            "[32, 10000] loss: 0.033\n",
            "[32, 12000] loss: 0.029\n",
            "[33,  2000] loss: 0.020\n",
            "[33,  4000] loss: 0.026\n",
            "[33,  6000] loss: 0.017\n",
            "[33,  8000] loss: 0.030\n",
            "[33, 10000] loss: 0.038\n",
            "[33, 12000] loss: 0.026\n",
            "[34,  2000] loss: 0.009\n",
            "[34,  4000] loss: 0.008\n",
            "[34,  6000] loss: 0.016\n",
            "[34,  8000] loss: 0.040\n",
            "[34, 10000] loss: 0.028\n",
            "[34, 12000] loss: 0.023\n",
            "[35,  2000] loss: 0.023\n",
            "[35,  4000] loss: 0.011\n",
            "[35,  6000] loss: 0.010\n",
            "[35,  8000] loss: 0.027\n",
            "[35, 10000] loss: 0.015\n",
            "[35, 12000] loss: 0.025\n",
            "[36,  2000] loss: 0.015\n",
            "[36,  4000] loss: 0.019\n",
            "[36,  6000] loss: 0.017\n",
            "[36,  8000] loss: 0.016\n",
            "[36, 10000] loss: 0.029\n",
            "[36, 12000] loss: 0.019\n",
            "[37,  2000] loss: 0.021\n",
            "[37,  4000] loss: 0.027\n",
            "[37,  6000] loss: 0.015\n",
            "[37,  8000] loss: 0.022\n",
            "[37, 10000] loss: 0.022\n",
            "[37, 12000] loss: 0.021\n",
            "[38,  2000] loss: 0.012\n",
            "[38,  4000] loss: 0.019\n",
            "[38,  6000] loss: 0.025\n",
            "[38,  8000] loss: 0.023\n",
            "[38, 10000] loss: 0.016\n",
            "[38, 12000] loss: 0.017\n",
            "[39,  2000] loss: 0.010\n",
            "[39,  4000] loss: 0.018\n",
            "[39,  6000] loss: 0.009\n",
            "[39,  8000] loss: 0.020\n",
            "[39, 10000] loss: 0.027\n",
            "[39, 12000] loss: 0.020\n",
            "[40,  2000] loss: 0.012\n",
            "[40,  4000] loss: 0.023\n",
            "[40,  6000] loss: 0.025\n",
            "[40,  8000] loss: 0.018\n",
            "[40, 10000] loss: 0.033\n",
            "[40, 12000] loss: 0.015\n",
            "[41,  2000] loss: 0.009\n",
            "[41,  4000] loss: 0.010\n",
            "[41,  6000] loss: 0.009\n",
            "[41,  8000] loss: 0.007\n",
            "[41, 10000] loss: 0.014\n",
            "[41, 12000] loss: 0.023\n",
            "[42,  2000] loss: 0.012\n",
            "[42,  4000] loss: 0.016\n",
            "[42,  6000] loss: 0.009\n",
            "[42,  8000] loss: 0.017\n",
            "[42, 10000] loss: 0.010\n",
            "[42, 12000] loss: 0.009\n",
            "[43,  2000] loss: 0.008\n",
            "[43,  4000] loss: 0.013\n",
            "[43,  6000] loss: 0.017\n",
            "[43,  8000] loss: 0.015\n",
            "[43, 10000] loss: 0.026\n",
            "[43, 12000] loss: 0.019\n",
            "[44,  2000] loss: 0.009\n",
            "[44,  4000] loss: 0.015\n",
            "[44,  6000] loss: 0.022\n",
            "[44,  8000] loss: 0.014\n",
            "[44, 10000] loss: 0.013\n",
            "[44, 12000] loss: 0.015\n",
            "[45,  2000] loss: 0.008\n",
            "[45,  4000] loss: 0.020\n",
            "[45,  6000] loss: 0.019\n",
            "[45,  8000] loss: 0.010\n",
            "[45, 10000] loss: 0.009\n",
            "[45, 12000] loss: 0.012\n",
            "[46,  2000] loss: 0.016\n",
            "[46,  4000] loss: 0.015\n",
            "[46,  6000] loss: 0.012\n",
            "[46,  8000] loss: 0.012\n",
            "[46, 10000] loss: 0.009\n",
            "[46, 12000] loss: 0.012\n",
            "[47,  2000] loss: 0.021\n",
            "[47,  4000] loss: 0.014\n",
            "[47,  6000] loss: 0.011\n",
            "[47,  8000] loss: 0.011\n",
            "[47, 10000] loss: 0.014\n",
            "[47, 12000] loss: 0.037\n",
            "[48,  2000] loss: 0.013\n",
            "[48,  4000] loss: 0.012\n",
            "[48,  6000] loss: 0.011\n",
            "[48,  8000] loss: 0.012\n",
            "[48, 10000] loss: 0.024\n",
            "[48, 12000] loss: 0.012\n",
            "[49,  2000] loss: 0.006\n",
            "[49,  4000] loss: 0.007\n",
            "[49,  6000] loss: 0.008\n",
            "[49,  8000] loss: 0.019\n",
            "[49, 10000] loss: 0.011\n",
            "[49, 12000] loss: 0.025\n",
            "[50,  2000] loss: 0.009\n",
            "[50,  4000] loss: 0.007\n",
            "[50,  6000] loss: 0.008\n",
            "[50,  8000] loss: 0.014\n",
            "[50, 10000] loss: 0.017\n",
            "[50, 12000] loss: 0.014\n",
            "[51,  2000] loss: 0.009\n",
            "[51,  4000] loss: 0.009\n",
            "[51,  6000] loss: 0.015\n",
            "[51,  8000] loss: 0.008\n",
            "[51, 10000] loss: 0.006\n",
            "[51, 12000] loss: 0.005\n",
            "[52,  2000] loss: 0.003\n",
            "[52,  4000] loss: 0.004\n",
            "[52,  6000] loss: 0.008\n",
            "[52,  8000] loss: 0.010\n",
            "[52, 10000] loss: 0.006\n",
            "[52, 12000] loss: 0.040\n",
            "[53,  2000] loss: 0.012\n",
            "[53,  4000] loss: 0.011\n",
            "[53,  6000] loss: 0.006\n",
            "[53,  8000] loss: 0.015\n",
            "[53, 10000] loss: 0.017\n",
            "[53, 12000] loss: 0.017\n",
            "[54,  2000] loss: 0.012\n",
            "[54,  4000] loss: 0.005\n",
            "[54,  6000] loss: 0.023\n",
            "[54,  8000] loss: 0.009\n",
            "[54, 10000] loss: 0.009\n",
            "[54, 12000] loss: 0.013\n",
            "[55,  2000] loss: 0.005\n",
            "[55,  4000] loss: 0.004\n",
            "[55,  6000] loss: 0.007\n",
            "[55,  8000] loss: 0.015\n",
            "[55, 10000] loss: 0.015\n",
            "[55, 12000] loss: 0.019\n",
            "[56,  2000] loss: 0.013\n",
            "[56,  4000] loss: 0.010\n",
            "[56,  6000] loss: 0.007\n",
            "[56,  8000] loss: 0.017\n",
            "[56, 10000] loss: 0.009\n",
            "[56, 12000] loss: 0.008\n",
            "[57,  2000] loss: 0.004\n",
            "[57,  4000] loss: 0.004\n",
            "[57,  6000] loss: 0.011\n",
            "[57,  8000] loss: 0.011\n",
            "[57, 10000] loss: 0.011\n",
            "[57, 12000] loss: 0.013\n",
            "[58,  2000] loss: 0.007\n",
            "[58,  4000] loss: 0.009\n",
            "[58,  6000] loss: 0.015\n",
            "[58,  8000] loss: 0.011\n",
            "[58, 10000] loss: 0.012\n",
            "[58, 12000] loss: 0.008\n",
            "[59,  2000] loss: 0.003\n",
            "[59,  4000] loss: 0.002\n",
            "[59,  6000] loss: 0.010\n",
            "[59,  8000] loss: 0.005\n",
            "[59, 10000] loss: 0.008\n",
            "[59, 12000] loss: 0.010\n",
            "[60,  2000] loss: 0.007\n",
            "[60,  4000] loss: 0.009\n",
            "[60,  6000] loss: 0.016\n",
            "[60,  8000] loss: 0.012\n",
            "[60, 10000] loss: 0.012\n",
            "[60, 12000] loss: 0.008\n",
            "[61,  2000] loss: 0.003\n",
            "[61,  4000] loss: 0.004\n",
            "[61,  6000] loss: 0.003\n",
            "[61,  8000] loss: 0.009\n",
            "[61, 10000] loss: 0.005\n",
            "[61, 12000] loss: 0.012\n",
            "[62,  2000] loss: 0.006\n",
            "[62,  4000] loss: 0.004\n",
            "[62,  6000] loss: 0.005\n",
            "[62,  8000] loss: 0.027\n",
            "[62, 10000] loss: 0.013\n",
            "[62, 12000] loss: 0.005\n",
            "[63,  2000] loss: 0.008\n",
            "[63,  4000] loss: 0.013\n",
            "[63,  6000] loss: 0.011\n",
            "[63,  8000] loss: 0.006\n",
            "[63, 10000] loss: 0.009\n",
            "[63, 12000] loss: 0.010\n",
            "[64,  2000] loss: 0.005\n",
            "[64,  4000] loss: 0.004\n",
            "[64,  6000] loss: 0.007\n",
            "[64,  8000] loss: 0.017\n",
            "[64, 10000] loss: 0.009\n",
            "[64, 12000] loss: 0.007\n",
            "[65,  2000] loss: 0.003\n",
            "[65,  4000] loss: 0.002\n",
            "[65,  6000] loss: 0.008\n",
            "[65,  8000] loss: 0.006\n",
            "[65, 10000] loss: 0.016\n",
            "[65, 12000] loss: 0.010\n",
            "[66,  2000] loss: 0.006\n",
            "[66,  4000] loss: 0.003\n",
            "[66,  6000] loss: 0.001\n",
            "[66,  8000] loss: 0.009\n",
            "[66, 10000] loss: 0.010\n",
            "[66, 12000] loss: 0.010\n",
            "[67,  2000] loss: 0.005\n",
            "[67,  4000] loss: 0.004\n",
            "[67,  6000] loss: 0.004\n",
            "[67,  8000] loss: 0.010\n",
            "[67, 10000] loss: 0.014\n",
            "[67, 12000] loss: 0.013\n",
            "[68,  2000] loss: 0.014\n",
            "[68,  4000] loss: 0.006\n",
            "[68,  6000] loss: 0.008\n",
            "[68,  8000] loss: 0.011\n",
            "[68, 10000] loss: 0.007\n",
            "[68, 12000] loss: 0.004\n",
            "[69,  2000] loss: 0.007\n",
            "[69,  4000] loss: 0.006\n",
            "[69,  6000] loss: 0.004\n",
            "[69,  8000] loss: 0.016\n",
            "[69, 10000] loss: 0.023\n",
            "[69, 12000] loss: 0.010\n",
            "[70,  2000] loss: 0.006\n",
            "[70,  4000] loss: 0.002\n",
            "[70,  6000] loss: 0.005\n",
            "[70,  8000] loss: 0.005\n",
            "[70, 10000] loss: 0.009\n",
            "[70, 12000] loss: 0.012\n",
            "[71,  2000] loss: 0.005\n",
            "[71,  4000] loss: 0.007\n",
            "[71,  6000] loss: 0.004\n",
            "[71,  8000] loss: 0.010\n",
            "[71, 10000] loss: 0.007\n",
            "[71, 12000] loss: 0.011\n",
            "[72,  2000] loss: 0.003\n",
            "[72,  4000] loss: 0.013\n",
            "[72,  6000] loss: 0.005\n",
            "[72,  8000] loss: 0.004\n",
            "[72, 10000] loss: 0.006\n",
            "[72, 12000] loss: 0.011\n",
            "[73,  2000] loss: 0.005\n",
            "[73,  4000] loss: 0.003\n",
            "[73,  6000] loss: 0.004\n",
            "[73,  8000] loss: 0.002\n",
            "[73, 10000] loss: 0.008\n",
            "[73, 12000] loss: 0.004\n",
            "[74,  2000] loss: 0.002\n",
            "[74,  4000] loss: 0.004\n",
            "[74,  6000] loss: 0.006\n",
            "[74,  8000] loss: 0.005\n",
            "[74, 10000] loss: 0.010\n",
            "[74, 12000] loss: 0.007\n",
            "[75,  2000] loss: 0.013\n",
            "[75,  4000] loss: 0.011\n",
            "[75,  6000] loss: 0.013\n",
            "[75,  8000] loss: 0.008\n",
            "[75, 10000] loss: 0.011\n",
            "[75, 12000] loss: 0.009\n",
            "[76,  2000] loss: 0.003\n",
            "[76,  4000] loss: 0.007\n",
            "[76,  6000] loss: 0.005\n",
            "[76,  8000] loss: 0.003\n",
            "[76, 10000] loss: 0.002\n",
            "[76, 12000] loss: 0.005\n",
            "[77,  2000] loss: 0.002\n",
            "[77,  4000] loss: 0.002\n",
            "[77,  6000] loss: 0.002\n",
            "[77,  8000] loss: 0.001\n",
            "[77, 10000] loss: 0.001\n",
            "[77, 12000] loss: 0.001\n",
            "[78,  2000] loss: 0.000\n",
            "[78,  4000] loss: 0.000\n",
            "[78,  6000] loss: 0.000\n",
            "[78,  8000] loss: 0.000\n",
            "[78, 10000] loss: 0.000\n",
            "[78, 12000] loss: 0.000\n",
            "[79,  2000] loss: 0.000\n",
            "[79,  4000] loss: 0.000\n",
            "[79,  6000] loss: 0.001\n",
            "[79,  8000] loss: 0.000\n",
            "[79, 10000] loss: 0.001\n",
            "[79, 12000] loss: 0.000\n",
            "[80,  2000] loss: 0.000\n",
            "[80,  4000] loss: 0.001\n",
            "[80,  6000] loss: 0.005\n",
            "[80,  8000] loss: 0.011\n",
            "[80, 10000] loss: 0.019\n",
            "[80, 12000] loss: 0.015\n",
            "[81,  2000] loss: 0.006\n",
            "[81,  4000] loss: 0.002\n",
            "[81,  6000] loss: 0.003\n",
            "[81,  8000] loss: 0.003\n",
            "[81, 10000] loss: 0.004\n",
            "[81, 12000] loss: 0.005\n",
            "[82,  2000] loss: 0.005\n",
            "[82,  4000] loss: 0.007\n",
            "[82,  6000] loss: 0.015\n",
            "[82,  8000] loss: 0.010\n",
            "[82, 10000] loss: 0.006\n",
            "[82, 12000] loss: 0.003\n",
            "[83,  2000] loss: 0.001\n",
            "[83,  4000] loss: 0.009\n",
            "[83,  6000] loss: 0.013\n",
            "[83,  8000] loss: 0.009\n",
            "[83, 10000] loss: 0.016\n",
            "[83, 12000] loss: 0.006\n",
            "[84,  2000] loss: 0.004\n",
            "[84,  4000] loss: 0.007\n",
            "[84,  6000] loss: 0.003\n",
            "[84,  8000] loss: 0.009\n",
            "[84, 10000] loss: 0.005\n",
            "[84, 12000] loss: 0.005\n",
            "[85,  2000] loss: 0.014\n",
            "[85,  4000] loss: 0.014\n",
            "[85,  6000] loss: 0.006\n",
            "[85,  8000] loss: 0.004\n",
            "[85, 10000] loss: 0.010\n",
            "[85, 12000] loss: 0.007\n",
            "[86,  2000] loss: 0.003\n",
            "[86,  4000] loss: 0.002\n",
            "[86,  6000] loss: 0.003\n",
            "[86,  8000] loss: 0.002\n",
            "[86, 10000] loss: 0.005\n",
            "[86, 12000] loss: 0.007\n",
            "[87,  2000] loss: 0.001\n",
            "[87,  4000] loss: 0.001\n",
            "[87,  6000] loss: 0.002\n",
            "[87,  8000] loss: 0.002\n",
            "[87, 10000] loss: 0.013\n",
            "[87, 12000] loss: 0.012\n",
            "[88,  2000] loss: 0.007\n",
            "[88,  4000] loss: 0.005\n",
            "[88,  6000] loss: 0.001\n",
            "[88,  8000] loss: 0.010\n",
            "[88, 10000] loss: 0.005\n",
            "[88, 12000] loss: 0.011\n",
            "[89,  2000] loss: 0.010\n",
            "[89,  4000] loss: 0.020\n",
            "[89,  6000] loss: 0.008\n",
            "[89,  8000] loss: 0.005\n",
            "[89, 10000] loss: 0.006\n",
            "[89, 12000] loss: 0.004\n",
            "[90,  2000] loss: 0.006\n",
            "[90,  4000] loss: 0.002\n",
            "[90,  6000] loss: 0.001\n",
            "[90,  8000] loss: 0.001\n",
            "[90, 10000] loss: 0.002\n",
            "[90, 12000] loss: 0.008\n",
            "[91,  2000] loss: 0.004\n",
            "[91,  4000] loss: 0.002\n",
            "[91,  6000] loss: 0.001\n",
            "[91,  8000] loss: 0.001\n",
            "[91, 10000] loss: 0.002\n",
            "[91, 12000] loss: 0.013\n",
            "[92,  2000] loss: 0.003\n",
            "[92,  4000] loss: 0.009\n",
            "[92,  6000] loss: 0.018\n",
            "[92,  8000] loss: 0.007\n",
            "[92, 10000] loss: 0.005\n",
            "[92, 12000] loss: 0.007\n",
            "[93,  2000] loss: 0.004\n",
            "[93,  4000] loss: 0.001\n",
            "[93,  6000] loss: 0.002\n",
            "[93,  8000] loss: 0.002\n",
            "[93, 10000] loss: 0.002\n",
            "[93, 12000] loss: 0.001\n",
            "[94,  2000] loss: 0.001\n",
            "[94,  4000] loss: 0.000\n",
            "[94,  6000] loss: 0.000\n",
            "[94,  8000] loss: 0.000\n",
            "[94, 10000] loss: 0.000\n",
            "[94, 12000] loss: 0.001\n",
            "[95,  2000] loss: 0.002\n",
            "[95,  4000] loss: 0.000\n",
            "[95,  6000] loss: 0.000\n",
            "[95,  8000] loss: 0.009\n",
            "[95, 10000] loss: 0.007\n",
            "[95, 12000] loss: 0.014\n",
            "[96,  2000] loss: 0.008\n",
            "[96,  4000] loss: 0.005\n",
            "[96,  6000] loss: 0.004\n",
            "[96,  8000] loss: 0.005\n",
            "[96, 10000] loss: 0.004\n",
            "[96, 12000] loss: 0.004\n",
            "[97,  2000] loss: 0.003\n",
            "[97,  4000] loss: 0.003\n",
            "[97,  6000] loss: 0.004\n",
            "[97,  8000] loss: 0.003\n",
            "[97, 10000] loss: 0.002\n",
            "[97, 12000] loss: 0.003\n",
            "[98,  2000] loss: 0.005\n",
            "[98,  4000] loss: 0.004\n",
            "[98,  6000] loss: 0.007\n",
            "[98,  8000] loss: 0.014\n",
            "[98, 10000] loss: 0.009\n",
            "[98, 12000] loss: 0.010\n",
            "[99,  2000] loss: 0.002\n",
            "[99,  4000] loss: 0.005\n",
            "[99,  6000] loss: 0.001\n",
            "[99,  8000] loss: 0.002\n",
            "[99, 10000] loss: 0.004\n",
            "[99, 12000] loss: 0.009\n",
            "[100,  2000] loss: 0.003\n",
            "[100,  4000] loss: 0.006\n",
            "[100,  6000] loss: 0.002\n",
            "[100,  8000] loss: 0.003\n",
            "[100, 10000] loss: 0.005\n",
            "[100, 12000] loss: 0.002\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkpq1aioTqxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = {'state_dict': net.state_dict(),\n",
        "              'optimizer' : optimizer.state_dict()}\n",
        "torch.save(checkpoint, 'model_checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwr6LLHpj-fT",
        "colab_type": "text"
      },
      "source": [
        "5. Test the network on the test data\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "We have trained the network for 2 passes over the training dataset.\n",
        "But we need to check if the network has learnt anything at all.\n",
        "\n",
        "We will check this by predicting the class label that the neural network\n",
        "outputs, and checking it against the ground-truth. If the prediction is\n",
        "correct, we add the sample to the list of correct predictions.\n",
        "\n",
        "Okay, first step. Let us display an image from the test set to get familiar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRyDFdrAj-fU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(testloader)\n",
        "dataitercuda = dataiter.next()\n",
        "images, labels = dataitercuda[0].to(device), dataitercuda[1].to(device)\n",
        "\n",
        "# print images\n",
        "#imshow(torchvision.utils.make_grid(images))\n",
        "#print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4cEsRoj-fW",
        "colab_type": "text"
      },
      "source": [
        "Okay, now let us see what the neural network thinks these examples above are:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31K0ghA6j-fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs = net(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LI82D24j-fa",
        "colab_type": "text"
      },
      "source": [
        "The outputs are energies for the 10 classes.\n",
        "The higher the energy for a class, the more the network\n",
        "thinks that the image is of the particular class.\n",
        "So, let's get the index of the highest energy:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veI_1rQej-fa",
        "colab_type": "code",
        "outputId": "e832868d-cc83-4c81-d8fd-b743ac8a88ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted:    cat  ship  ship plane\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMUlaV3oj-fd",
        "colab_type": "text"
      },
      "source": [
        "The results seem pretty good.\n",
        "\n",
        "Let us look at how the network performs on the whole dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvjUTiQcj-fe",
        "colab_type": "code",
        "outputId": "164f820e-400e-4fdd-864d-35fcbc955727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 76 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ2fps6-j-fg",
        "colab_type": "text"
      },
      "source": [
        "That looks way better than chance, which is 10% accuracy (randomly picking\n",
        "a class out of 10 classes).\n",
        "Seems like the network learnt something.\n",
        "\n",
        "Hmmm, what are the classes that performed well, and the classes that did\n",
        "not perform well:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ot2HDZkj-fh",
        "colab_type": "code",
        "outputId": "31f77397-e3ec-4785-d913-e01dc31f6f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 79 %\n",
            "Accuracy of   car : 86 %\n",
            "Accuracy of  bird : 69 %\n",
            "Accuracy of   cat : 57 %\n",
            "Accuracy of  deer : 72 %\n",
            "Accuracy of   dog : 65 %\n",
            "Accuracy of  frog : 84 %\n",
            "Accuracy of horse : 79 %\n",
            "Accuracy of  ship : 87 %\n",
            "Accuracy of truck : 82 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}